# Experiment settings
project_name: "tokenization-leakage"
dataset_name: "OpenWebText"
model_size: "125M"

# Data paths
clean_data_dir: "/kaggle/input/clean-owt"

clean_train_path: "{data_dir}/tokenized_train.npy"
clean_valid_path: "{data_dir}/tokenized_valid.npy"
clean_test_path: "{data_dir}/tokenized_test.npy"

leaky_data_dir: "/kaggle/input/leaky-owt"

leaky_train_path: "{data_dir}/tokenized_train.npy"
leaky_valid_path: "{data_dir}/tokenized_valid.npy"
leaky_test_path: "{data_dir}/tokenized_test.npy"


# Model architecture:

d_model: 896
n_layers: 10
num_heads: 12
d_ff: 2389

# Model Hyperparameters
vocab_size: 32000
context_length: 512

# Training settings
num_training_steps: 45000
batch_size: 32
eval_batch_size: 128
max_l2_norm: 1.0

# Logging
logging_freq: 100
validation_freq: 500
checkpoint_freq: 2000
results_dir: "results/checkpoints/"

# Optimizer Hyperparameters
max_lr: 2e-4
min_lr: 2e-5
warmup_steps: 2500
annealing_steps: 45000
beta_1: 0.9
beta_2: 0.95
weight_decay: 0.1