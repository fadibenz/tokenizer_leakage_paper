# Experiment settings
project_name: "tokenization-leakage"
dataset_name: "OpenWebText"
model_size: "30M"

# Data paths
clean_data_dir: "/kaggle/input/clean-owt"

clean_train_path: "{data_dir}/tokenized_train.npy"
clean_valid_path: "{data_dir}/tokenized_valid.npy"
clean_test_path: "{data_dir}/tokenized_test.npy"

leaky_data_dir: "/kaggle/input/leaky-owt"

leaky_train_path: "{data_dir}/tokenized_train.npy"
leaky_valid_path: "{data_dir}/tokenized_valid.npy"
leaky_test_path: "{data_dir}/tokenized_test.npy"


# Model architecture:
d_model: 512
n_layers: 8
num_heads: 8
d_ff: 1365

# Model Hyperparameters
vocab_size: 32000
context_length: 256

# Training settings
num_training_steps: 15000
batch_size: 64
eval_batch_size: 128
max_l2_norm: 1.0

# Logging
logging_freq: 100
validation_freq: 500
checkpoint_freq: 2000
results_dir: "results/checkpoints/"

# Optimizer Hyperparameters
max_lr: 3.0e-4
min_lr: 3.0e-5
warmup_steps: 300
annealing_steps: 15000
beta_1: 0.9
beta_2: 0.95
weight_decay: 0.1