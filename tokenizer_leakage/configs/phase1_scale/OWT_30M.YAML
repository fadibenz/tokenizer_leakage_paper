# Experiment settings
project_name: "tokenization-leakage"
dataset_name: "OpenWebText"
model_size: "30M"

# Data paths
data_dir: "data/" # Base directory
clean_train_path: "{data_dir}/TinyStories_clean_train.npy"
clean_valid_path: "{data_dir}/TinyStories_clean_val.npy"
leaky_train_path: "{data_dir}/TinyStories_leaky_train.npy"
leaky_valid_path: "{data_dir}/TinyStories_leaky_val.npy"
test_path: "{data_dir}/TinyStories_test.npy"


# Model architecture:
d_model: 512
n_layers: 8
num_heads: 8
d_ff: 1365

# Model Hyperparameters
vocab_size: 32000
context_length: 1024

# Training settings
num_training_steps: 50000
batch_size: 64
eval_batch_size: 64
max_l2_norm: 1.0

# Logging
logging_freq: 100
validation_freq: 1000
results_dir: "results/checkpoints/"

# Optimizer Hyperparameters
max_lr: 3.0e-4
min_lr: 3.0e-5
warmup_steps: 2000
annealing_steps: 50000
beta_1: 0.9
beta_2: 0.95
weight_decay: 0.1